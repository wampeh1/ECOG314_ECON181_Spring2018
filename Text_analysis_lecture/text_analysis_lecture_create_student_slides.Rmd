---
title: "Text Analysis in R: Student Slides"
author: "Mandy Bowers"
date: "April 20, 2018"
output: beamer_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, size = "tiny",
                      fig.height = 2.5,
                      fig.width = 4.5,
                      fig.align = "center", width = 50, 
                      tidy.opts=list(width.cutoff=40),tidy=TRUE)

library(ggplot2)
library(scales)
library(stringr)
library(dplyr)
#install.packages("corpus")
library(corpus)
#install.packages("tm")
library(tm)
#install.packages("tidytext")
library(tidytext)
#install.packages("tidyr")
library(tidyr)
#install.packages("wordcloud")
library(wordcloud)
#install.packages("reshape2")
library(reshape2)
#install.packages("widyr")
library(widyr)
#install.packages("topicmodels")
library(topicmodels)
#install.packages("RColorBrewer")
library(RColorBrewer)

setwd("/if/fame/eme/usr/mandy/Howard/text_analysis")
```

## Text Analysis in R


### What is text analysis?


### How is text analysis currently being used?




## Text Analysis in R


### What is text analysis?

* Text analysis allows you to extract key information from unstructured text and organize it into a usable way for analysis.

### How is text analysis currently being used?

\centerline{\includegraphics[height=1in]{./Reference/investors_make_money.png}}


\centerline{\includegraphics[height=1in]{./Reference/investors_text.png}}


## Text analysis and the Fed

\centerline{\includegraphics[height=1in]{./Reference/fed_text_analysis.png}}


\includegraphics[height=2in]{./Reference/sentiment_analysis_fed.png}
\includegraphics[height=2in]{./Reference/word_freq_fed.png}

## Introduction to the Dataset

* Today we'll be working with FOMC meeting transcripts.
    + Source: https://www.federalreserve.gov/monetarypolicy/fomc_historical_year.htm

* What is the FOMC? Why is it important?

## What is the FOMC?

* The Federal Open Market Committee is the monetary policymaking body of the Federal Reserve System.

* To help achieve the Fed's objectives, the FOMC will adjust the level of the short-term interest rate to respond to changes in the economic outlook.
    + What are the Fed's objectives?
    + What other measures has the FOMC used to achieve its objectives?

* The FOMC usually has 12 members - 7 members of the Board of Governors and 5 of the 12 Reserve Bank Presidents.

* The FOMC schedules 8 meetings per year, about every 6 weeks. After each meeting, the FOMC issues a policy statement to summarize the Committee's economic outlook and policy decision.

## Introduction to the Dataset: FOMC Transcripts

* Three weeks after each meeting, a full set of minutes for the meeting are published, and complete transcripts are published five years after the meeting.

* First we'll work with a very limited sample to get a feel for text analysis before we take on a lot of data:
    + Feb 1-2, 2005
    + Jan 27-28, 2009

## Looking at our data: The Corpus

Using R's 'tm'(text mining) package to open our 'corpus'.

\scriptsize
```{r, echo=TRUE, collapse=TRUE}
docs <- tm::VCorpus(DirSource("Sources/FOMC_ex1"))
class(docs)
glimpse(docs)

```

## Looking at our data: The Text

What does it look like inside our corpus? Let's find out.

\scriptsize
```{r, echo=TRUE, collapse=TRUE, results='asis'}
##docs[1][[1]]$content
docs[1][[1]]$content[1]
docs[1][[1]]$content[4]
docs[1][[1]]$content[705]
```

## Preparing the Text: Question

* We need to make our text more machine-friendly. What do you think we need to do to this text to make it machine readable?


## Preparing the Text: Answer

* We need to make our text more machine-friendly. What do you think we need to do to this text to make it machine readable?
    + Remove punctuation
    + Remove case
    + Remove numbers
    + Remove white space
    + Remove 'useless' words = **stopwords**
    + Remove useless endings on words ("ing" "s" etc.) = **stemming words**

## Preparing the Text: the TM package

Fortunately, we have one package prepared to help us take care of all of these things

Text cleaning task | TM package solution
-------------------|--------------------
Remove punctuation | removePunctuation
Make lowercase     | content_transformer(tolower)
Remove numbers     | removeNumbers
Remove stopwords   | removeWords, stopwords("english") 
Stem words         | stemDocument
Remove white space | stripWhitespace

These are used as arguments in tm_map(*corpus to edit*, *option*)


## Preparing the Text: using the TM package to clean text (Example)

\scriptsize
```{r, eval=TRUE, echo=TRUE, collapse=TRUE, results='asis'}
docs[1][[1]]$content[4]
docs1 <- tm_map(docs, removePunctuation)
docs1[1][[1]]$content[4]

```

## Preparing the Text: Cleaning Text (Exercise)

\scriptsize
```{r, eval=FALSE, echo=TRUE, warning=FALSE}
## Transform the capitalization to lowercase
docs2 <- tm_map(docs1,                        )
#View the content to make sure it worked!
docs2[1][[1]]$content

## Remove the numbers from the documents
docs3 <- tm_map(       ,               )
docs3[1][[1]]$

## Remove stopwords from the documents
docs_cleaned <- tm_map(       ,        ,            )
docs_cleaned[ ][[ ]]$
```

\scriptsize
```{r, eval=TRUE, include = FALSE}
## Transform the capitalization to lowercase
docs2 <- tm_map(docs1, content_transformer(tolower))
#View the content to make sure it worked!
docs2[1][[1]]$content[4]

## Remove the numbers from the documents
docs3 <- tm_map(docs2, removeNumbers)
docs3[1][[1]]$content[4]

## Remove stopwords from the documents
docs4 <- tm_map(docs3, removeWords, stopwords("english"))
docs4[1][[1]]$content[4]

```


## Preparing the Text: Finishing up our text cleaning

* What does stemming do? Why is this useful?

\scriptsize
```{r, eval=TRUE, echo=TRUE, collapse=TRUE, results='asis'}
docs_cleaned <- tm_map(docs4, stripWhitespace)
docs_stemmed <- tm_map(docs_cleaned, stemDocument)
docs_stemmed[1][[1]]$content[4]

```


## Putting our Corpus into the Tidyverse

* How can we 'tidy' our corpus? With `tidy` from our friend, the tidyverse!

```{r, eval = FALSE, collapse=TRUE}
tidy_docs <- broom::tidy(docs_stemmed)

# What class is tidy_docs?
class(tidy_docs)

tidy_docs
tidy_docs$text

```

## Unnest_tokens

* The power of the tidyverse is harnessed by splitting each word into one datapoint
* We can split a document using `unnest_tokens`

```{r, eval = FALSE, collapse=TRUE}
tidy_words <- tidy_docs %>% unnest_tokens(word, text)
# We should take a look at our words
# Can anyone tell me what the most frequent words are?
tidy_words$word

```


```{r, eval=TRUE, include = FALSE}
tidy_docs <- tidy(docs_stemmed)
tidy_words <- tidy_docs %>% unnest_tokens(word, text)

```


## Graphing with Tidytext: Code

* Obviously, there's a better way.
* Let's use our old friend `ggplot` to analyze our text

```{r, eval=FALSE, echo=TRUE, results='asis'}
word_freq <- tidy_words %>% 
  dplyr::count(word, sort = TRUE) %>%
  filter(n > 50) %>%
  mutate(word = reorder(word, n)) %>% 
  head(10) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()
word_freq
```

## Graphing with Tidytext: Plot

*Is this really the output I want?

\scriptsize
```{r, eval=TRUE, echo=FALSE}
word_freq <- tidy_words %>% 
  dplyr::count(word, sort = TRUE) %>% 
  filter(n > 50) %>% 
  mutate(word = reorder(word, n)) %>% 
  head(10) %>% 
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()
word_freq
```

## Removing Unwanted Words: Revisiting Joins

* Let's get rid of some of these less interesting words!
* I have a set of unwanted words. Which join could I use to take them out of my `tidy_words`?

\centerline{\includegraphics[height=2.4in]{./Reference/joins.png}}

## Removing Unwanted Words: Creating our 'Y'

* `tidy_words` will be our X, and we want to remove a set of words Y
* Let's first create our Y based on the boring words that were showing up in the graph:
* What data class will Y have to be?

\scriptsize
```{r, eval=TRUE, echo=TRUE, collapse=TRUE, results='asis'}
remove_words <- c("mr", "go", "one", "us", "like", "will", "can", "just", "also", "now", "chairman", "thank", "vice")
rm_words_table <- data.frame(remove_words)
colnames(rm_words_table) <- c("word")
```

## Removing Unwanted Words: Using anti_join

* Now let's anti_join our two dataframes, `tidy_words` and `rm_words_table`

\scriptsize
```{r, eval=TRUE, echo=TRUE, collapse=TRUE, results='asis'}
interesting_tidy_words <- tidy_words %>% 
  anti_join(rm_words_table)
```

## Graphing with Tidytext: Exercise

* Fill in the graph code below based on our previous word frequency graph code

\scriptsize
```{r, eval=FALSE}
interesting_word_freq <-                  %>% 
  count(     , sort = TRUE) %>% 
  filter(  > 50) %>% 
         (word = reorder(word, n)) %>% 
  head(  ) %>% 
  ggplot(aes(word,  )) +
  geom_col() +
  xlab(NULL) + coord_flip()
interesting_word_freq
```

\scriptsize
```{r, eval=TRUE, include=FALSE}
interesting_word_freq <- interesting_tidy_words %>% 
  count(word, sort = TRUE) %>% 
  filter(n > 50) %>% 
  mutate(word = reorder(word, n)) %>% 
  head(15) %>% 
  ggplot(aes(word, n)) + geom_col() +
  xlab(NULL) + coord_flip()
interesting_word_freq
```

## Comparing Articles' Word Frequency

* So far we've been looking at our two different articles together. How can we compare how words changed between 2005 and 2009?

* Let's remind ourselves what `interesting_tidy_words` has in it.

```{r, eval=TRUE, echo=TRUE, collapse=TRUE, results='asis'}
colnames(interesting_tidy_words)
```

* Which one of these will allow us to distinguish between article names? (Hint: Check `interesting_tidy_words` in view!)

## Manipulating the ID: Case When

* The current IDs we have are long and complicated -- let's switch the names using `case_when`


```{r, eval=TRUE, echo=TRUE, collapse=TRUE, warning=FALSE, results='asis'}
words_0509 <- interesting_tidy_words %>%
  select(word, id) %>%
  mutate(meeting = 
           case_when(id == 'FOMC20050202meeting.txt' ~ 'fomc2005', 
                    id == "FOMC20090128meeting.txt" ~ 'fomc2009')) %>%
  select(-id)
```

## Graphing Articles' Word Frequency: Preparing the Data

\scriptsize
```{r, eval=TRUE, echo=TRUE, collapse=TRUE, warning=FALSE}
frequency <- words_0509 %>% 
  count(meeting, word) %>% #create col 'n' - times per meeting a word appears
  group_by(meeting) %>% #tells mutate we will sum on 'meeting'
  mutate(proportion = n / sum(n)) %>% #calculate a column with times 
  # a word appears divided by the total words in given meeting
  select(-n) %>% #remove extraneous count column
  spread(meeting, proportion) %>%  #reorganizes data around 'word' 
  # with meeting as column and proportion inside frame
  arrange(desc(fomc2005)) #lets make it easy to look at most common words

head(frequency,5)
```

## Graphing Articles' Word Frequency: Creating the Graph

* What will this graph look like?

\scriptsize
```{r, eval=FALSE, echo=TRUE, collapse=TRUE, warning=FALSE, results='asis'}
ggplot(frequency, 
    aes(x = fomc2005, y = fomc2009, color = 
          abs(fomc2005-fomc2009))) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, 
              width = 0.3, height = 0.3) +
  geom_text(aes(label = word), 
            check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(limits = c(0, 0.001), 
                       low = "darkslategray4", high = "gray75") +
  theme(panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(),
        panel.background = element_blank(), 
        axis.line = element_line(colour = "black"), 
        legend.position="none") +
  labs(y = "FOMC 2009", x = "FOMC 2005")
```

## Graphing Articles' Word Frequency: Creating the Graph

\centerline{\includegraphics[height=3in]{./Reference/fomc05vs09.png}}


## Word Clouds: Introduction

* That graph was interesting, but it was a little hard to read. Besides, we're most interested in what words are appearing when...introducting wordclouds!

* Let's create a word cloud for 2005 together, and then you can make one for the 2009 words

```{r, eval=FALSE, echo=TRUE, collapse=TRUE, warning=FALSE}
word_cloud_05 <- words_0509 %>% 
  filter(meeting=='fomc2005') %>% 
  count(word, sort = TRUE) %>% 
  with(wordcloud(word, n, max.words = 80, colors=brewer.pal(8,"Dark2")))
```

## Word Clouds: Output

\centerline{\includegraphics[height=3in]{./Reference/wordcloud2005.png}}

## Word Clouds: Exercise

* Now use the template below to create a wordcloud for 2009! (it's ok if R gets mad at you because things can't fit on the screen. Also, you may have to zoom out a bit to make sure all the words have room to appear in Rmarkdown)

```{r, eval=FALSE, echo=TRUE, collapse=TRUE, warning=FALSE}
word_cloud_09 <-           %>% 
  filter(               ) %>% 
  count(word,      = TRUE) %>% 
  with(wordcloud(       ,   , max.words =    ,
                        colors=brewer.pal(8,"Dark2")))
```

## Comparing Word Clouds

* Let's compare the most common words in the early 2005/2009 FOMC meetings

\includegraphics[height=2in]{./Reference/wordcloud2005.png}
\includegraphics[height=2in]{./Reference/wordcloud2009.png}

## Sentiment Analysis: Introduction

* Another popular method of text analysis is opinion mining/sentiment analysis. When we read text we understand whether it is positive/negative (plus additional emotions). How can we make a computer understand that?

* A common approach to sentiment analysis that makes use of the tidyverse is to measure the 'sentiment' of each individual word and combine all the words of the texts to create a score for the overall text or portions of the text.

* To this end, the tidytext package contains several sentiment lexicons in the `sentiments` database (bing, nrc, afinn). Note that these datasets were generally constructed via crowdsourcing or present-day individuals. Topic-specific dictionaries are a hot topic in text analysis.

## Sentiment Analysis: Bing

* Today, we'll just look at one, `bing`. Explore `bing` with

\scriptsize
```{r, eval=TRUE, echo=TRUE, collapse=TRUE, warning=FALSE}
sentiment_index <- get_sentiments("bing")
sentiment_index
```

\normalsize

* How does `bing` categorize words? 

* Can you foresee any problems with using a dataset like this to analyze text?

## Sentiment Analysis: Drivers

* Let's look at what words will be driving the sentiment of the 2009 meeting.

\scriptsize
```{r, eval=FALSE, echo=TRUE, collapse=TRUE, warning=FALSE, results='asis'}
sentiment_09 <- words_0509 %>% 
  filter(meeting=='fomc2009') %>% 
  ## Let's just look at 2009 words
  inner_join(sentiment_index) %>% 
  ## How are we using an inner join? What is a downside to an inner join?
  count(word, sentiment, sort=TRUE) %>%
  ungroup()
```

## Sentiment Analysis: Drivers Results

\scriptsize
```{r, eval=TRUE, echo=FALSE}
sentiment_09 <- words_0509 %>% 
  filter(meeting=='fomc2009') %>% 
  inner_join(sentiment_index) %>% 
  count(word, sentiment, sort=TRUE) %>%
  ungroup()
sentiment_09
```

## Sentiments over the Long Run

* A more interesting question is how does the sentiment of these meetings change over time. How do you interpret this graph?

\includegraphics[height=2.8in]{./Reference/sentiment_over_time.png}


## Importing our Second Corpus

* For this we'll need more than just two meetings. Let's create a larger dataset.

* Take a look at this data using glimpse and then in the view finder.

\scriptsize
```{r, eval=FALSE, echo=TRUE, collapse=TRUE}
fomc <- tm::VCorpus(DirSource("Sources/FOMCminutes"))
glimpse(fomc)
```

\scriptsize
```{r, eval=TRUE, include=FALSE}
fomc <- tm::VCorpus(DirSource("Sources/FOMCminutes"))
```

\normalsize

* We have to clean these documents up too! What's a way we can expedite this for the future?

## Writing Functions to Clean Text

* What do these functions do?

\scriptsize
```{r, eval=TRUE, echo=TRUE}
id_tidy <- function(corpus){
  for (i in 1:nrow(corpus)){
    temp <- str_extract_all(corpus$id[i],"\\(?[0-9]+\\)?")
    temp <- temp[[1]][1]
    temp2a <- substring(temp,1,4)
    temp2b <- substring(temp,5,6)
    temp3 <- paste(temp2a, temp2b, sep = "-")
    corpus$id[i] <- temp3
  }
  return(corpus)
}

prep_text <- function(corpus){
  corpus <- corpus %>% 
    tm_map(removeNumbers) %>% 
    tm_map(removePunctuation) %>% 
    tm_map(content_transformer(tolower)) %>% 
    tm_map(removeWords, stopwords("english")) %>% 
    tm_map(removeWords, remove_words) %>% 
    tm_map(stemDocument) %>% 
    tidy() %>% 
    id_tidy() %>% 
    select(id, text)
}
```

## Running our Custom Functions

* Run these functions to get our new data ready to use

```{r, eval=TRUE, echo=TRUE}
tidy_fomc <- prep_text(fomc)
tidy_fomc
```

## Looking at the Change in Sentiment over Time (Exercise)

* Fill out the code below to create a chart tracking change over time in sentiment of FOMC meeting minutes.

\scriptsize
```{r, eval=FALSE, echo=TRUE, collapse=TRUE, results='asis'}
tidy_fomc_words <- tidy_fomc %>% unnest_tokens(     , text)
fomc_sentiment <- tidy_fomc_words %>% 
           (sentiment_index) %>% 
  count(index = id, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)

ggplot(              , aes(index, sentiment, fill="   ")) +
  geom_col(show.legend = FALSE) + 
  theme(panel.background = element_rect(fill = "white", colour = NA), 
  panel.border = element_rect(fill = NA, colour = "grey50"),
  axis.text.x = element_text(angle =       , hjust= 1, vjust = 1)) +
  labs(                                    ) 
```

## Looking at the Change in Sentiment over Time (Image)

\includegraphics[height=2.8in]{./Reference/sentiment_over_time.png}

## Words over Time: Data Prep (Exercise)

* Now we're going to look at how the frequency of individual words changed over the period

* We are going to look at 6 words very important to the Fed and the Great Recession: inflation, unemployment, rate, mortgage, bank, risk

* First we need a summary word counts by FOMC meeting (this is recorded in a column name)

    + Hint: Look at `tidy_fomc` in the viewer to learn what the column names are

```{r, eval=FALSE, echo=TRUE, results='asis'}
fomc_words_count <- tidy_fomc %>% 
  unnest_tokens(     ,      ) %>% 
  group_by(    ) %>% 
  count(word)
```

```{r, eval=TRUE, include=FALSE}
fomc_words_count <- tidy_fomc %>% 
  unnest_tokens(word, text) %>% 
  group_by(id) %>% 
  count(word)
```

## Words over Time: Create Yearly Totals

* I don't want to have to look at a bajillion meetings. I just want to look at a summary for the year. Let's consolidate our FOMC meetings by year.

\scriptsize
```{r, eval=TRUE, echo=TRUE, collapse=TRUE, results='asis'}
fomc_year_term_counts <- fomc_words_count %>%
    #extracts just year from the meeting id
    tidyr::extract(id, "year", "(\\d+)", convert = TRUE) %>% 
    group_by(year, word) %>% 
  #collapses all appearances of a word in a given year
    summarize(word_total_by_year = sum(n)) %>%
    group_by(year)%>% 
  #provides column of total word count per year
    mutate(year_total = sum(word_total_by_year))
```

## Words over Time: Let's look at our data

\scriptsize
```{r, eval=TRUE, echo=TRUE}
fomc_year_term_counts %>% filter(word == "rate")
```

* How many times does the word 'bank' appear in 2008?
* How many times does the word 'risk' appear in 2007?
* How many times does the word 'inflation' appear in 2006?


## Tracking Individual Words over Time: Exercise

* Now, use this code to create a six facet graph tracking the changes in inflation, umeployment, rate, risk, mortgage, and bank from 2005 to 2009.

* Hints: 
    + `%in%` will help as such: 'category' `%in%` c("items you want from category")
    + We want to plot year against a word's appearance rate, where a word's appearance rate is the word's mentions per year divided by the total number of words said in that year

\scriptsize
```{r, eval=FALSE, echo=TRUE, collapse=TRUE, results='asis'}
fomc_year_term_counts %>%
  filter(      %in% c("inflat",      ,      ,      , "mortgag",     )) %>%
  ggplot(aes(     ,          /       )) +
  geom_point() +
  geom_smooth() +
  facet_wrap(~      , scales = "free_y") +
  scale_y_continuous(labels = scales::percent_format()) +
  ylab("% Frequency of word in FOMC minutes")+
  theme(strip.text.x = element_text(size =   ))
```

## Tracking Individual Words over Time: Results

* Congratulations, you can now quantify some of the evolution in the Fed's monetary policy thinking!

\includegraphics[height=2.5in]{./Reference/word_freq_culm.png}

## Summary: Text Analysis in the Tidyverse

* Today we were introduced to some of R's text analysis tools
* What path did we take today?
* What names of packages do you recognize here?

\centerline{\includegraphics[height=2.5in]{./Reference/tidyverse.png}}
