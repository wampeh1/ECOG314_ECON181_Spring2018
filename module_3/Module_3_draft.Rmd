---
title: "Module 3"
output: pdf_document
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = F, error = T,
                      fig.height = 3.5,
                      fig.width = 5)

invisible(library(tidyverse))
invisible(library(ggthemes))
invisible(library(data.table))
invisible(library(stargazer))

# Set your working directory to be the mod3 folder
# setwd()
```

Goals of module 3:  
* Regression analysis overview
    * Understanding purpose and how to use it to address your questions
* Relationship between variable types and regression interpretation
  * Stock and flow variables
  * Logging data
  * Lagging and differencing data
    * Lag() and data.table::shift()
* Special considerations for:
    * Policy evaluation
      * Difference-in-differences
    * Time series data
      * Panels
    * Survey data
      * Recap on using dummies
* Controlling analysis output
    * Stargazer options
    * Broom package
* Making maps
    * ggmap
* Regression
    * Variable transformation
        * Log of price
        * price per square foot (?)
        * Interaction effects (?) - may not find any significant ones
        
# Day One

For the past month we've looked at housing data from Redfin to see how housing prices in the Washington, D.C. metro area differ based on distances to public transit. This module will shift attention to employment and take us through a variety of numerical analyses starting with policy evaluation. We will examine how minimum wage laws affect labor conditions in local areas using data collected by David Card and Alan Krueger on fast-food job wages.

The 1994 Card and Krueger Study
  * A 1992, $0.80 minimum wage increase by New Jersey prompted researchers to revisit the debate on the aggregate effect of the minimum wage on labor markets.
  * Card and Krueger collected information from fast-food restaurants in New Jersey and eastern Pennsylvania.
  * They found no indication of a reduction in employment.
  * An even bigger impact of the study was the introduction of differences-in-differences.
    * Intuitive tool suitable for quasi-experimental studies

First, a refresher on when to use regressions.

When to use regression analysis in economics
  * Trying to identify causation
    * Correlation vs. causation
  * **Inductive** reasoning
    * Inference based on observed data

Regression analysis defined
  * Used to describe the relationship between:
    * A single response variable Y and
    * Any number of predictor variables X_1, X_2, ..., X_n
  * The goal is to show Y is determined by X_1, ..., X_n
  * The response variable for OLS must be continuous (but sometimes economists cheat)
  * There are no restrictions on the predictor variables
    * Predictors can be continuous, discrete, or categorical

Steps to take before you put your data into a regression
  * Check for:
    * Missing values
    * Outliers
    * Asymmetric distributions
    * Clustering of values
    * Unexpected patterns
  * Numerical summaries
    * Mean, min, max, variance, etc.
    * Correlations
  * Graphical summaries
    * Scatter plots
    * Histograms
    * Box plots

We are going to start by reading in the Card and Kreuger data and doing some basic analysis in the same way that we have been since the beginning of class. We want to get an idea of what the data looks like before we perform any sort of regressions.

Find the data file with the name 'public', and you will see this is saved as a .dat file: a generic data file format that can be read into R using the "fread" function from the data.table package. Once you've read in the data, take a look at the variable names. 

```{r}
# read in the employment data
cardKrueger <- fread("Data/public.dat")

# look at how reading in data went
glimpse(cardKrueger)
```

The variable names do not appear very helpful, and many of our variables were read in as factors. To understand how our dataset is set up, we will need to look at the codebook. You can open up the codebook in R from the file panel. We will need to rename and recode our variables to match the codebook so that we can more easily keep track of variables in our dataset.
  * Can you identify the variables that would be best treated as categorical? 
  * How about numerical?

*Why did many of our variables get read in as factors?*

```{r}
# try reading in the employment data again, but avoid variables that should not be factors getting turned into factors
cardKrueger <- fread("Data/public.dat", na.strings = c("NA", "."))

# change variable names according to code book
names(cardKrueger) <- c("SHEET", "CHAIN", "CO_OWNED", "STATE", "SOUTHJ", 
                        "CENTRALJ", "NORTHJ", "PA1", "PA2", "SHORE", "NCALLS", 
                        "EMPFT", "EMPPT", "NMGRS", "WAGE_ST", "INCTIME", 
                        "FIRSTINC", "BONUS", "PCTAFF", "MEALS", "OPEN", 
                        "HRSOPEN", "PSODA", "PFRY", "PENTREE", "NREGS", 
                        "NREGS11", "TYPE2", "STATUS2", "DATE2","NCALLS2", 
                        "EMPFT2", "EMPPT2", "NMGRS2", "WAGE_ST2", "INCTIME2",
                        "FIRSTIN2", "SPECIAL2", "MEALS2", "OPEN2R", "HRSOPEN2", 
                        "PSODA2","PFRY2", "PENTREE2", "NREGS2", "NREGS112")

```

Now that the variable names match our variables in the codebook, we can transform some of the variable values according to the codebook. Start by looking at the class of our variables using one of the functions we learned during the first module. 

```{r}
# Check variable classes
glimpse(cardKrueger)

# create a clean dataset using information from the code book
cardKrueger_clean <- cardKrueger %>% 
  mutate(chainName = case_when(CHAIN == 1 ~ "Burger King",
                               CHAIN == 2 ~ "KFC",
                               CHAIN == 3 ~ "Roys",
                               CHAIN == 4 ~ "Wendys"), 
         stateAbbrv = ifelse(STATE == 1, "NJ", "PA"),
         location = case_when(SOUTHJ == 1 ~ "Southern NJ",
                              CENTRALJ == 1 ~ "Central NJ",
                              NORTHJ == 1 ~ "Northern NJ",
                              PA1 == 1 ~ "Northeast suburbs of Philadelphia",
                              PA2 == 1 ~ "Easton",
                              SHORE == 1 ~ "NJ Shore")) %>%
  # subset to useful variables
  select(stateAbbrv, chainName, location, EMPFT, EMPPT, NMGRS, WAGE_ST, HRSOPEN, 
         PSODA, PFRY, PENTREE, EMPFT2, EMPPT2, NMGRS2, WAGE_ST2, HRSOPEN2, 
         PSODA2, PFRY2, PENTREE2)

# Tidy up workspace
rm(cardKrueger)
```

### Understanding the movement in wages

What is the average wage by state, by fast food chain, before and after the policy goes into effect?

```{r}
# use dplyr to check average wage by state and chain
avgWage <- cardKrueger_clean %>% 
  group_by(chainName, stateAbbrv) %>%
  summarise(avgWageBefore = mean(WAGE_ST, na.rm = TRUE), 
            avgWageAfter = mean(WAGE_ST2, na.rm = TRUE)) %>%
  # Sort by chain to get like-restaurants next to each other
  arrange(chainName) %>%
  # Remove grouping attribute
  ungroup()
avgWage
```

There seems to be significant movement in starting wage between these two periods in New Jersey, while Pennsylvania stays relatively constant for each chain. Is this enough to know whether the minimum wage increase in New Jersey had an impact on the labor market?
  * Do we see differences between chains in the wages paid to employees?
    * Consider how this may or may not impact our analysis
  * Do you think stores with higher salaries have cheaper or more expensive food?
  * What does this table tell us about employment?

Make a histogram plot of wages in the two periods and use color to indicate the time period.
  * Pay attention to the shape of the data and how that impacts ggplot layering

```{r}
# try out ggthemes to change the look of the plot
ggplot(cardKrueger_clean) +
  # Need to use geom_ in the following way due to the structure of our data
  geom_histogram(bins = 15, aes(x = WAGE_ST, fill = "Pre"), alpha = 0.5) +
  geom_histogram(bins = 15, aes(x = WAGE_ST2, fill = "Post"), alpha = 0.5) +
  labs(x = "Starting Wage",
     y = "Density",
     fill = "Period",
     title = "Starting Wages in Pre- and Post-Periods") +
  theme_few() +
  theme(legend.position = "bottom", legend.title = element_text(size = 7)) +
  guides(fill = guide_legend(title.position = "top", title.hjust = 0.5))

```

Evident from the plot is the effect of the policy on wages. What are some important nuances in the graph important for our research topic?

### Understanding the simultaneous shifts in employment levels

What is the total number of employees by state, by fast food chain, before and after the policy goes into effect?

```{r}
# use dplyr to check number of employees by state and chain
numEmp <- cardKrueger_clean %>%
  group_by(chainName, stateAbbrv) %>%
  # Card and Krueger weigh managers half as much as other employees
  summarise(empBefore = sum(EMPFT, EMPPT, 0.5*NMGRS, na.rm = TRUE),
            empAfter = sum(EMPFT2, EMPPT2, 0.5*NMGRS2, na.rm = TRUE)) %>%
  # Sort by chain to get like-restaurants next to each other
  arrange(chainName) %>%
  # Remove grouping attribute
  ungroup()
numEmp
```

Make a histogram of employment by region before the minimum wage changes and after.

```{r}
# reshape the data to make an indicator variable for the time period
empl_PrePost <- cardKrueger_clean %>%
  gather(-chainName, -stateAbbrv, -location, 
         key = "variable", value = "value") %>%
  filter(variable %in% c("EMPFT", "EMPFT2")) %>%
  mutate(period = ifelse(variable == "EMPFT2", "post", "pre")) %>%
  mutate(period = factor(period, levels = c("pre", "post")))

# plot the data
ggplot(empl_PrePost) +
  geom_histogram(bins = 10, aes(x = value, fill = location), alpha = 0.8) +
  labs(x = "Number of Full Time Employees",
       y = "Density",
       fill = "Region",
       title = "Pre- and Post-Period Full Time Employment") +
  scale_fill_economist() +
  theme_economist() + 
  # We can use multiple calls to theme
  theme(legend.position = "bottom", legend.text = element_text(size = 8), 
        legend.title = element_text(size = 8)) +
  facet_grid(. ~ period) +
  guides(fill = guide_legend(nrow = 2, byrow = TRUE, title.position = "top", title.hjust = 0.5))

```

We have seen through our data exploration exercise a number of relationships between variables of interest. Different states had different patterns in wage changes, chains seemed to have different levels in the wages offered to new employees, and regions have varying levels of employees per store. What we have not seen, however, is any proof of causal effect.

Our research goal lends itself to causal inference. Surely, we do care about movements in wages and employment in general; our goal is to exactly find the effect of one on the other, and in particular among low-wage jobs (which is the only type of job we observe in this dataset). Now we turn to regressions.

## OLS and Fixed Effects

```{r}
# straightforward linear regression on pre-period employment and wages
preReg <- lm(EMPFT ~ WAGE_ST, cardKrueger_clean)
```

preReg contains the output of a regression that proposed the following hypothesis: full-time employment is linearly determined by starting wages (plus unobserved noise). Let's see how well this hypothesis performed on the pre-policy-change period. You have already seen the stargazer package at this point, and we will continue to use it here.

```{r}
# Call stargazer to create output table; type = "text" will let us review it right in the markdown file
stargazer(preReg, type = "text")
```

Wage is coming out weakly significant as a determinant of employment; however, the model as a whole does not do the greatest job in explaining variations in the employment variable.
  * What other variables did we find to be impactful on employment?
    * Are any of them categorical variables?

## Fixed effects
You already have some exposure to the use of fixed effects from module 2, so the goal here will be to build an intuition for recognizing when you can and cannot employ fixed effects and how to construct variables that will make it easy to do so. 

Fixed effects let you limit your analysis to within-group variation. In a panel data setting this is most commonly used to account for heterogeneity **between** individual subjects, resulting in a model that can get you consistent estimates even with structural differences between groups/subjects.
  * As an analogy, think about how group_by() in the dplyr package works.
    * Similar to running lm() after using group_by()

For our use cases, there is no meaningful differences between a fixed effects estimator and a least squares dummy variables estimator; they are computationally equivalent. This means we will think of indvidual/group fixed effects and the use of dummies as similar or identical.

Some reasons to use fixed effects:
  * The variation you see when pooling all observations together might not be of interest to you.
  * You believe a variable, or any transformation of one, should not enter your model linearly but still does explain variation.
  * There is a non-numeric variable that you want to include in your model.

When you cannot employ fixed effects with a variable:
  * There is not enough variation in each group of the variable

*As an example, could we use a fixed effect with a different group for each observation of our dataset?* 
How many observations would each group have (is this enough variation to run a regression)?

Specifying fixed effects in lm():
```{r echo=TRUE, eval=FALSE}
# If class of x is character, factor, logical, or otherwise only has two levels
lm(y ~ x)

# If class of x is numeric, convert to factor
lm(y ~ as.factor(x))
```

```{r}
# adding state fixed effects on pre-period variables
preReg_fixedState <- lm(EMPFT ~ WAGE_ST + stateAbbrv, 
                          cardKrueger_clean)
stargazer(preReg_fixedState, type = "text")
```

## Stargazer Options Review

We have seen and used stargazer for over a month, but to recap: the purpose of stargazer is to create publication-ready tables. Both regression tables and summary tables can be made in the same, user-friendly format. Virtually any number of model objects (the type of output from lm()) for regression tables or any number of data frames/vectors/matrices for summary tables can be input into stargazer. Let us see what happens when we add a second lm object to the stargazer call from the first regression we ran; i.e. let's add both preReg and preReg_fixedState to the same stargazer call.

```{r}
stargazer(preReg, preReg_fixedState, type = "text")
```

Two columns of coefficients appear in the regression table, the first belonging to the model used in preReg and the second from the model in preReg_fixedState. Using the summary tables feature of stargazer is very similar. Now let us try one of the data frames we created earlier when we were exploring data, numEmp. Recall that this data frame already contains summary statistics that we calculated; therefore, we want to specify to stargazer that we do not want to compute summary statistics as we make the table (since we already have the statistics we want). The way to handle this option is with the summary argument: setting summary = FALSE will command stargazer to output the content of the data frame.

```{r}
stargazer(numEmp, type = "text", summary = FALSE)
```

You have already explored and used many of these options, but here is a list of some commonly used arguments for stargazer:

Main arguments used for regression tables:
  * covariate.labels: Labels for the variables listed on the far left, top to bottom
  * dep.var.labels: Labels for the variables listed at the top of each column, left to right
  * title: Title of table
  * keep/omit: Lets you suppress printing out variables you do not want, based on text patterns
  * order: Lets you control the order of dependent variables (from top to bottom), based on text patterns
  * intercept.bottom: If set to FALSE, the intercept term is put at the top; the default is TRUE and the intercept is listed last
  * column.labels: Labels for columns. These show up right below dep.var.labels and let you provide a very short description of the model in that column.
  * ...: And many more!

Main arguments used for summary statistics tables:
  * summary: If TRUE, calculate number of observations, mean, median, max, and min of numeric variables in the input object. If FALSE, output the contents of the input object.
  * rownames: If TRUE, output the rownames (by default these are the row number). If FALSE, suppress rownames from the output.
  * digits: Lets you control how many decimal places to print.
  * ...: Some more!
  
To find a description of EVERY argument to stargazer try the manual.
  * ?stargazer

```{r}
# adding fixed effects and controls on pre-period variables
preReg_controls <- lm(EMPFT ~ WAGE_ST + stateAbbrv + HRSOPEN + 
                        PSODA + PFRY + PENTREE, cardKrueger_clean)
stargazer(preReg, preReg_fixedState, preReg_controls, type = "text", intercept.bottom = FALSE)

# simple linear regression on post-period variables
PostReg <- lm(numFullTimeEmp2 ~ startingWage2, cardKrueger_clean)
stargazer(PostReg, type = "text")

# adding fixed effects on post-period variables
PostReg_fixedEffects <- lm(numFullTimeEmp2 ~ startingWage2 + state, 
                          cardKrueger_clean)
stargazer(PostReg_fixedEffects, type = "text")

# adding fixed effects and controls on post-period variables
PostReg_controls <- lm(numFullTimeEmp2 ~ startingWage2 + state + hoursOpen2 + 
                        priceSoda2 + priceFry2 + priceEntree2, cardKrueger_clean)
stargazer(PostReg_controls, type = "text")
```
HOW MUCH OF THE WAGE CAN YOU EXPLAIN USING LOCATION? WHAT ABOUT CHAIN?


## Difference-in-differences (DID)

With the DID technique you are able to make use of a treatment and a control groups to make causal inferences about the effect of some treatment. You cannot call on this tool if you do not observe both groups; the treatment group so you can observe the result of treatment and the control group so you have a baseline comparison to estimate the full effect of the treatment.
  * The control should serve as a counterfactual - it would tell you how the treatment group would look like if no treatment ever occurred.
  * Admissible data structures include aggregated data, pooled cross-sections, or longitudinal
A useful tool to use in quasi-experiments, DID addresses concerns about random assignment (since it is not necessary) while also accounting for confounders.
  * Confounders are variables that affect both sides of your equation.
  * The major assumption: parallel trends
    * Differences in original characteristics between control and treatment groups okay - differences in _trends_ between groups _NOT_ okay
    * Violation of parallel trends leads to biased estimates

```{r echo=FALSE}
didData <- data_frame(Year = rep(2000:2004, 2),
                      Treatment = c(rep("Treated", 5), rep("Control", 5)),
                      Y = c(3:5, 7, 9, 1:5))
counterfac <- data_frame(Year = rep(2000:2004, 2),
                         Y = rep(3:7, 2))
ggplot(didData, aes(x = Year, y = Y, colour = as.factor(Treatment))) +
  geom_line(lwd = 1.5) +
  labs(x = NULL, y = "Outcome", colour = "Treatment", title = "Example of Parallel Trends") +
  geom_line(aes(x = counterfac$Year, y = counterfac$Y), linetype = 4, lwd = 0.5) +
  geom_vline(aes(xintercept = 2002), linetype = 5) +
  theme_hc()

#Tidy up workspace
rm(didData)
rm(counterfac)
```

The additional difference in the treatment group in the post-treatment period (after 2002) from the baseline counterfactual (the dotted line) is the DID estimate. 

```{r}
# recreate the differences-in-differences done in the Card and Krueger paper

# create a total employment variable: part time employees count as half, and
## include number of managers as full time employees. Then find the change
## in total employment between periods
regression_data <-
  cardKrueger_clean %>%
  mutate(totalEmp = numFullTimeEmp + numManagers + 0.5*numPartTimeEmp,
         totalEmp2 = numFullTimeEmp2 + numManagers2 + 0.5*numPartTimeEmp2) %>%
  mutate(changeTotalEmp = totalEmp2 - totalEmp)

# find the average employment in both periods and the average change
empAvgs <- group_by(regression_data, state) %>% 
  summarise(preAvgEmp = mean(totalEmp, na.rm = TRUE), 
            postAvgEmp = mean(totalEmp2, na.rm = TRUE),
            avgEmpChange = mean(changeTotalEmp, na.rm = TRUE))
empAvgs
```

# Day Two

## Lagging data
What you may very well encounter in the next dataset or model you use, sometimes a situation arises when one variable is influenced by the **lag** of another variable or even by its own lag.

```{r, echo=FALSE}
unlagged <- data_frame(Bank = c(rep("A", 3), rep("B", 3)),
                       Year = rep(1990:1992, 2),
                       Y = round(rnorm(6, 0, 5), 0),
                       X1 = sample(1:20, 6))

lagged <- unlagged %>%
  group_by(Bank) %>%
  mutate(Y_lag = data.table::shift(Y, n = 1, type = "lag"),
         X1_lag = data.table::shift(X1, n = 1, type = "lag")) %>%
  select(Bank, Year, Y, Y_lag, X1, X1_lag)

stargazer::stargazer(unlagged, lagged, summary = FALSE, type = "text", rownames = FALSE)

#Tidy up workspace
rm(unlagged)
rm(lagged)
```

Two lagging tools (among many)
  * Lag(x, k, ...) - base R
    * x is the numerical series you want to lag
    * k is the number of periods to lag by
  * Shift(x, n, type) - data.table
    * x is again the numerical series
    * n is the number of periods
    * type is whether you want to "lead" or "lag" and takes a character


```{r, message = F}
unlagdf <- data_frame(x = 1:5)

lagdf <- unlagdf %>%
  mutate(x_lag = lag(x, 1))
lagdf
```

**Question:** How might you lag a variable in a *panel*  or *cross-sectional* dataset? 

Let's try creating a lagged variable in cardKrueger.

# Day Three

# Day Four
